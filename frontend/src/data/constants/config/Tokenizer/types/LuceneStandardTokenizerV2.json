{
  "discriminatorValue": "#Microsoft.Azure.Search.StandardTokenizerV2",
  "label": "LuceneStandardTokenizerV2",
  "description": "Breaks text following the Unicode Text Segmentation rules. Implemented using Apache Lucene.",
  "fields": [
    {
      "key": "maxTokenLength",
      "label": "Max Token Length",
      "type": "number",
      "default": 255,
      "min": 1,
      "max": 300,
      "tooltip": "The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters."
    }
  ]
}
