{
  "analyzers": "The analyzers for the index.",
  "name": "The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.",
  "@odata.type": "A URI fragment specifying the type of analyzer.",
  "tokenizer": "The name of the tokenizer to use to divide continuous text into a sequence of tokens, such as breaking a sentence into words.",
  "charFilters": "A list of character filters used to prepare input text before it is processed by the tokenizer. The filters are run in the order in which they are listed.",
  "tokenFilters": "A list of token filters used to filter out or modify tokens generated by a tokenizer. The filters are run in the order in which they are listed.",
  "maxTokenLength": "The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.",
  "stopwords": "A list of stopwords.",
  "pattern": "A regular expression pattern to match token separators. Default is an expression that matches one or more non-word characters (\\\\W+).",
  "lowercase": "A value indicating whether terms should be lower-cased. Default is true.",
  "flags": "Regular expression flags."
}
